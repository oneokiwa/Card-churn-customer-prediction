#===================================================================================
# ğŸ“Œ  MLP íŒŒì´í”„ë¼ì¸ ìš”ì•½
# 1ï¸âƒ£. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# 2ï¸âƒ£. MLP ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ
# 3ï¸âƒ£. ëª¨ë¸ í‰ê°€ ë° ì§€í‘œ ì¶œë ¥
# 4ï¸âƒ£. Permutation Importance ë¶„ì„
# 5ï¸âƒ£. K-Means í´ëŸ¬ìŠ¤í„°ë§
# 6ï¸âƒ£. í´ëŸ¬ìŠ¤í„°ë³„ ì´íƒˆë¥  ë¶„ì„
# 7ï¸âƒ£. SHAP ë¶„ì„
# 8ï¸âƒ£. ì”ì•¡ êµ¬ê°„ë³„ ì´íƒˆë¥  ë¶„ì„

# ë°ì´í„° ë¶„ë¥˜, ì¤‘ìš” ë³€ìˆ˜ ë¶„ì„, ê³ ê° êµ°ì§‘í™”, ì„¤ëª…ë ¥ ë¶„ì„ê¹Œì§€ ì¢…í•©ì ìœ¼ë¡œ ìˆ˜í–‰
#===================================================================================

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from sklearn.inspection import permutation_importance

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import seaborn as sns
import shap
import matplotlib.pyplot as plt

# ëª¨ë¸ ê°œì„ ì„ ìœ„í•œ ì¶”ê°€ ì¡°ì¹˜ (ê³¼ì í•© ë°©ì§€)
# 1. Early Stopping ì¶”ê°€
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

from tensorflow.keras.regularizers import l2


# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
X_train = pd.read_csv('train/X_train.csv')
X_val = pd.read_csv('train/X_val.csv')
X_test = pd.read_csv('train/X_test.csv')
y_train = pd.read_csv('train/y_train.csv').squeeze()
y_val = pd.read_csv('train/y_val.csv').squeeze()
y_test = pd.read_csv('train/y_test.csv').squeeze()

# ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”© (Label Encoding)
def preprocess_features(df):
    for col in df.select_dtypes(include=['object']).columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
    return df

X_train = preprocess_features(X_train)
X_val = preprocess_features(X_val)
X_test = preprocess_features(X_test)

# ë°ì´í„° ì •ê·œí™” (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)


###  MLP(ë‹¤ê³„ì¸µ í¼ì…‰íŠ¸ë¡ ) ëª¨ë¸ êµ¬ì¶• ###
# ì…ë ¥ì¸µ: íŠ¹ì„± ê°œìˆ˜ë§Œí¼ ë‰´ëŸ° í¬í•¨
# ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ: 64ê°œ ë‰´ëŸ°, ReLU í™œì„±í™” í•¨ìˆ˜, Dropout(0.3) ì ìš©
# ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ: 32ê°œ ë‰´ëŸ°, ReLU í™œì„±í™” í•¨ìˆ˜, Dropout(0.3) ì ìš©
# ì¶œë ¥ì¸µ: 1ê°œ ë‰´ëŸ°, sigmoid í™œì„±í™” í•¨ìˆ˜ (ì´ì§„ ë¶„ë¥˜)
# ì¸ê³µì‹ ê²½ë§ ëª¨ë¸ ì •ì˜ (MLP)

# ëª¨ë¸ ì„±ëŠ¥ ê°œì„ 
model = Sequential([     # Sequential ëª¨ë¸ ì‚¬ìš©
    keras.Input(shape=(X_train_scaled.shape[1],)),
    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),  # L2 ì •ê·œí™” ì¶”ê°€, ReLU í™œì„±í™” í•¨ìˆ˜ :  ë¹„ì„ í˜•ì„±ì„ ìœ ì§€
    Dropout(0.3),
    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),  # L2 ì •ê·œí™” ì¶”ê°€
    Dropout(0.3),
    Dense(1, activation='sigmoid') # sigmoid ì‚¬ìš©, ì¶œë ¥ê°’ì„ 0~1 ì‚¬ì´ì˜ í™•ë¥ ë¡œ ë³€í™˜
                                   # 0.5 ì´ìƒì´ë©´ ì´íƒˆ (1)
])

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

###  ëª¨ë¸ í•™ìŠµ  ###
# Adam ì˜µí‹°ë§ˆì´ì €, binary_crossentropy ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©
# epochs=50, batch_size=32
# í•™ìŠµ ê³¼ì •ì—ì„œ ì†ì‹¤(Loss) ë° ì •í™•ë„(Accuracy) ê·¸ë˜í”„ ì¶œë ¥

history = model.fit(X_train_scaled, y_train,
                    epochs=50,
                    batch_size=32,
                    validation_data=(X_val_scaled, y_val),
                    verbose=1,
                    callbacks=[early_stopping])  # Early Stopping ì ìš©

############ í•™ìŠµ ê²°ê³¼ ì‹œê°í™” (Loss & Accuracy) ##############

# ì†ì‹¤(Loss) ê·¸ë˜í”„
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title("Loss over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

# ì •í™•ë„(Accuracy) ê·¸ë˜í”„
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title("Accuracy over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

plt.show()

# ëª¨ë¸ í‰ê°€ (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
y_test_pred = (model.predict(X_test_scaled) > 0.5).astype(int)

# ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ì¶œë ¥
accuracy = accuracy_score(y_test, y_test_pred)
auc = roc_auc_score(y_test, model.predict(X_test_scaled))
classification = classification_report(y_test, y_test_pred)

print(f"Test Accuracy: {accuracy:.4f}")
print(f"ROC AUC Score: {auc:.4f}")
print("Classification Report:")
print(classification)


# # ğŸ“Œ Permutation Importanceë¥¼ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤ ìƒì„±
class KerasWrapper:
    def __init__(self, model):
        self.model = model

    def fit(self, X, y):
        """Sklearn ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•´ fit ë©”ì„œë“œ ì¶”ê°€ (ì‹¤ì œë¡œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)"""
        pass

    def predict(self, X):
        """MLP ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ 0 ë˜ëŠ” 1ë¡œ ë³€í™˜"""
        return (self.model.predict(X, verbose=0) > 0.5).astype(int).flatten()

# ğŸ“Œ Sklearn ì¸í„°í˜ì´ìŠ¤ë¥¼ ë”°ë¥´ëŠ” MLP ë˜í¼ ëª¨ë¸ ìƒì„±
wrapped_model = KerasWrapper(model)

# ğŸ“Œ Permutation Importance ê³„ì‚°
result = permutation_importance(
    estimator=wrapped_model,  # MLP ëª¨ë¸ ë˜í¼ ì „ë‹¬
    X=X_test_scaled,
    y=y_test,
    scoring="accuracy",
    n_repeats=10,
    random_state=42
)

# ğŸ“Œ ë³€ìˆ˜ ì¤‘ìš”ë„ ì‹œê°í™”
feature_importances = result.importances_mean
sorted_idx = np.argsort(feature_importances)[::-1]
top_n = 10  # ìƒìœ„ 10ê°œ ë³€ìˆ˜ë§Œ í‘œì‹œ

plt.figure(figsize=(20, 10))
plt.barh(X_test.columns[sorted_idx][:top_n], feature_importances[sorted_idx][:top_n])
plt.xlabel("Permutation Importance")
plt.title("MLP Feature Importance")
plt.gca().invert_yaxis()  # í° ê°’ì´ ìœ„ë¡œ ê°€ë„ë¡ ì„¤ì •
plt.show()

# ì¤‘ìš”í•œ ë³€ìˆ˜ ì„ íƒ
important_features = [
    "201809_ConsecutiveNonPerformanceMonths_Basic_24M_Card",
    "201809_UsageAmount_Credit_B0M",
    "201809_Balance_B0M",
    "201809_BillingAmount_B0",
    "201809_BillingAmount_R3M"
]

########### K-Means ë¶„ì„ ###########
# êµ°ì§‘í™”ë¥¼ ìœ„í•œ ë°ì´í„° ì„ íƒ (ì¤‘ìš” ë³€ìˆ˜ë§Œ ì‚¬ìš©)
cluster_data = X_test_scaled[:, [X_test.columns.get_loc(col) for col in important_features]]

# ìµœì ì˜ K ì°¾ê¸° (Elbow Method)
inertia = []
K_range = range(2, 10)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(cluster_data)
    inertia.append(kmeans.inertia_)

# Elbow Method ê·¸ë˜í”„ ì¶œë ¥
plt.figure(figsize=(8, 5))
plt.plot(K_range, inertia, marker='o')
plt.xlabel("Number of Clusters")
plt.ylabel("Inertia")
plt.title("Elbow Method for Optimal K")
plt.show()

# ìµœì ì˜ K=3ìœ¼ë¡œ K-Means ì ìš©
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters = kmeans.fit_predict(cluster_data)

# PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™” (2D ë³€í™˜)
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(cluster_data)

# êµ°ì§‘í™” ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(10, 6))
sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=clusters, palette='viridis')
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.title("Customer Segmentation using K-Means")
plt.legend(title="Cluster")
plt.show()


######### ì¶”ê°€ë¶„ì„ #########
# K-Means ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
X_test_clustered = pd.DataFrame(X_test_scaled, columns=X_test.columns)
X_test_clustered['Cluster'] = clusters
X_test_clustered['EXIT'] = y_test.values  # ì‹¤ì œ ì´íƒˆ ì—¬ë¶€ ì¶”ê°€

# í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ì´íƒˆìœ¨ ê³„ì‚°
cluster_exit_rates = X_test_clustered.groupby('Cluster')['EXIT'].mean()

# ì‹œê°í™”
plt.figure(figsize=(8, 5))
sns.barplot(x=cluster_exit_rates.index, y=cluster_exit_rates.values, palette='viridis')
plt.xlabel("Cluster")
plt.ylabel("EXIT Rate")
plt.title("EXIT Rate per Cluster")
plt.show()

######## shap ë¶„ì„ #########
# Cluster 0ì˜ ë°ì´í„°ë§Œ í•„í„°ë§
cluster_0_data = X_test_clustered[X_test_clustered['Cluster'] == 0]
cluster_0_features = cluster_0_data.drop(columns=['Cluster', 'EXIT'])  # ë…ë¦½ë³€ìˆ˜ë§Œ ì‚¬ìš©

# SHAP ë¶„ì„ì„ ìœ„í•œ Explainer ìƒì„±
explainer = shap.Explainer(model, cluster_0_features)
shap_values = explainer(cluster_0_features)

# SHAP Summary Plot (ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„)
shap.summary_plot(shap_values, cluster_0_features, feature_names=cluster_0_features.columns)

######### ì”ì•¡ êµ¬ê°„ ë¶„ì„ ##########
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# í´ëŸ¬ìŠ¤í„° 0 ë°ì´í„° í•„í„°ë§
cluster_0_data = X_test_clustered[X_test_clustered['Cluster'] == 0]

# ì”ì•¡ ë³€ìˆ˜ ì„ íƒ
balance_var = "201809_Balance_B0M"

# ğŸ“Œ ì”ì•¡ êµ¬ê°„ ë‚˜ëˆ„ê¸° (3ê°œ êµ¬ê°„: ë‚®ìŒ, ì¤‘ê°„, ë†’ìŒ)
bins = [0, 7500000, 15000000, 22588073]  # 3ê°œì˜ êµ¬ê°„ (ì¡°ì • ê°€ëŠ¥)
labels = ["Low (0 ~ 7.5M)", "Medium (7.5M ~ 15M)", "High (15M ~ 22.5M)"]

cluster_0_data['Balance_Bin'] = pd.cut(cluster_0_data[balance_var], bins=bins, labels=labels, include_lowest=True)

# ğŸ“Œ ê° êµ¬ê°„ë³„ ì´íƒˆìœ¨(Exit Rate) ê³„ì‚°
balance_exit_rates = cluster_0_data.groupby('Balance_Bin')['EXIT'].mean()

# ğŸ“Œ ì´íƒˆìœ¨ ì‹œê°í™” (Bar Plot)
plt.figure(figsize=(12, 7))
sns.barplot(x=balance_exit_rates.index, y=balance_exit_rates.values, palette='coolwarm')
plt.xlabel("Balance Range")
plt.ylabel("EXIT Rate")
plt.title(f"EXIT Rate by {balance_var} in Cluster 0")
plt.ylim(0, 1)  # 0~100% ë¹„ìœ¨ í‘œí˜„
plt.show()